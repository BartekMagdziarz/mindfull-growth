Epic 3 – LLM-Powered Journal Reflection Chat

1. Goal

Add an AI-powered chat feature to the Journaling Editor that allows users to have guided conversations with a Large Language Model (LLM) about their journal entries. After writing and tagging an entry, users can initiate a chat session with different therapeutic intentions (e.g., reflection, perspective shifts, proactive planning, identifying thinking traps). The chat conversations are saved and associated with the journal entry, enhancing the user's ability to gain deeper insights from their journaling practice.

⸻

2. Scope

This epic covers:

	1.	Chat Intentions & System Prompts
	•	Define a set of predefined chat intentions that guide the LLM's behavior:
	•	"Reflect" – Help the user reflect on their entry, explore deeper meanings, and understand their emotional patterns.
	•	"Help see differently" – Guide the user to consider alternative perspectives and challenge assumptions.
	•	"Help to be proactive" – Assist the user in identifying actionable steps and proactive solutions.
	•	"Thinking traps" – Help identify cognitive distortions and unhelpful thinking patterns.
	•	"Custom" – Allow the user to provide their own prompt/intention for the conversation.
	•	Each intention has a corresponding system prompt that sets the LLM's role and conversation style.
	•	System prompts should be designed to be therapeutic, supportive, and aligned with the app's goal of helping users gain insights from their journaling practice.
	•	The system prompts should instruct the LLM to:
	•	Use the journal entry's title, body, emotions, and tags as context.
	•	Keep conversations concise and focused (e.g., 3-5 exchanges).
	•	Ask thoughtful, open-ended questions when appropriate.
	•	Be empathetic and non-judgmental.
	•	Respect user privacy and avoid making clinical diagnoses.

	2.	Journal Editor UI Enhancements
	•	Add a "Chat" button to the bottom action bar in JournalEditorView.vue, positioned to the left of the "Cancel" button.
	•	Add a dropdown menu (or similar selection UI) that appears when the user hovers/clicks near the Chat button, showing the available chat intentions:
	•	"Reflect"
	•	"Help see differently"
	•	"Help to be proactive"
	•	"Thinking traps"
	•	"Custom"
	•	Each option should have a brief description (e.g., "Reflect – Explore deeper meanings and patterns").
	•	The Chat button should be:
	•	Disabled if the entry body is empty (same validation as Save button).
	•	Styled consistently with the existing Cancel and Save buttons (using AppButton component).
	•	When clicked (after selecting an intention):
	•	Save the entry first (same logic as Save button, but without navigating away).
	•	Store the entry ID in a way that can be accessed by the Chat view (e.g., via route params or a store).
	•	Navigate to the Chat interface view.

	3.	Chat Interface View
	•	Create a new view component: ChatView.vue (or JournalChatView.vue).
	•	Route: /journal/:id/chat (where :id is the journal entry ID).
	•	UI/behavior:
	•	Top app bar:
	•	Back navigation (to /journal/:id/edit or /journal).
	•	Title: "Chat about entry" or similar, with the selected intention displayed (e.g., "Reflect – Chat about entry").
	•	Content area:
	•	Display a brief summary of the entry context at the top (non-editable):
	•	Entry title (or "Untitled entry").
	•	Selected emotions (as chips).
	•	Selected tags (people and context, as chips).
	•	A short preview of the entry body (first 2-3 lines, or ~150 characters).
	•	Chat message area:
	•	Display conversation messages in a scrollable container.
	•	Each message shows:
	•	Sender (User or AI/Assistant).
	•	Message content.
	•	Timestamp (optional, or just show relative time like "Just now").
	•	Style user messages and AI messages differently (e.g., user messages on the right, AI on the left, or stacked vertically with distinct styling).
	•	Input area at the bottom:
	•	Text input field for user messages.
	•	Send button.
	•	Loading indicator when AI is responding.
	•	Actions:
	•	"Save conversation" button (enabled after at least one exchange):
	•		- Saves the chat session to the journal entry.
	•		- Persists the conversation to the database.
	•		- Navigates back to journal editor or journal list.
	•		- Shows success feedback (e.g., snackbar message).
	•	"End chat" or "Leave without saving" button:
	•		- Allows users to exit the chat interface without saving.
	•		- Discards the current conversation (it will not be added to the entry).
	•		- Shows a confirmation dialog if there are unsaved messages (to prevent accidental loss).
	•		- Navigates back to journal editor or journal list.
	•		- The conversation is not persisted and will be lost.
	•	Design should be coherent with the current journal editor design:
	•	Use the same Material Design–inspired components (AppCard, AppButton).
	•	Use the same color scheme and spacing.
	•	Maintain consistent typography and visual hierarchy.

	4.	LLM Integration & API Service
	•	Create an LLM service module (e.g., llmService.ts or openaiService.ts) that:
	•	Handles communication with the OpenAI API.
	•	Manages API key retrieval (from user settings/store).
	•	Constructs the conversation payload:
	•	System prompt (based on selected intention).
	•	Initial context message containing the journal entry data (title, body, emotions, tags).
	•	Conversation history (user and assistant messages).
	•	Sends requests to the OpenAI API (using gpt-4o-mini as the default model).
	•	Handles API errors gracefully (e.g., invalid API key, rate limits, network errors).
	•	Returns streaming or non-streaming responses (start with non-streaming for simplicity, can add streaming in a future story).
	•	Error handling:
	•	Show user-friendly error messages (e.g., "Invalid API key", "Network error", "Rate limit exceeded").
	•	Allow users to retry failed requests.
	•	Log errors for debugging (without exposing sensitive data).

	5.	Chat Data Model & Storage
	•	Define a chat data model:
	•	Each journal entry can have zero or more chat sessions associated with it.
	•	Each chat session contains:
	•	id: string (UUID)
	•	journalEntryId: string (reference to the journal entry)
	•	intention: string (e.g., "reflect", "help-see-differently", "proactive", "thinking-traps", "custom")
	•	customPrompt?: string (only if intention is "custom")
	•	createdAt: string (ISO timestamp)
	•	messages: Array<{
	•		role: "user" | "assistant"
	•		content: string
	•		timestamp: string (ISO timestamp)
	•	}>
	•	Storage decision: Add a new field to the JournalEntry domain model:
	•	chatSessions?: ChatSession[] (optional array of chat sessions)
	•	This approach is recommended because:
	•	Pros:
	•		- Clean separation between user content and AI-generated conversations.
	•		- Allows multiple chat sessions per entry (user can have different conversations at different times).
	•		- Better for future LLM analysis (can process chats separately from journal content).
	•		- Maintains data integrity and makes it easier to query/analyze chats.
	•		- Supports future features like chat history, exporting chats, or comparing different chat sessions.
	•	Cons:
	•		- Requires a database migration to add the new field.
	•		- Slightly more complex to display in views (need to handle array of chats).
	•	Implementation:
	•		- Update JournalEntry interface in domain/journal.ts to include chatSessions field.
	•		- Create a ChatSession type/interface in a new domain file (e.g., domain/chatSession.ts).
	•		- Update the database schema (Dexie) to support the new field (add a migration).
	•		- The migration should handle existing entries by initializing chatSessions as an empty array.
	•		- Update journalDexieRepository to handle the new field (no special logic needed, Dexie will store it automatically).

	6.	Chat Store & State Management
	•	Create a Pinia store (e.g., useChatStore.ts) that:
	•	Manages the current chat session state (active conversation, selected intention, etc.).
	•	Maintains temporary/unsaved conversation state (messages that haven't been saved yet).
	•	Provides actions:
	•	startChatSession(entryId, intention, customPrompt?)
	•	sendMessage(entryId, message)
	•	saveChatSession(entryId, chatSession) – persists the conversation to the journal entry.
	•	discardChatSession() – clears the current unsaved conversation without persisting.
	•	loadChatSessionsForEntry(entryId)
	•	The store should interact with:
	•	The LLM service to send messages and receive responses.
	•	The journal store to save chat sessions to entries (only when explicitly saved).
	•	Handle loading states, errors, and conversation history.
	•	Important: Unsaved conversations exist only in memory and are lost when the user navigates away without saving.

	7.	Profile View – API Key Management
	•	Update ProfileView.vue to include:
	•	A section for "AI Settings" or "LLM Configuration".
	•	An input field for the OpenAI API key:
	•	Label: "OpenAI API Key"
	•	Type: password (masked input for security).
	•	Placeholder: "sk-..." or "Enter your OpenAI API key".
	•	Help text: Brief explanation of why the API key is needed and how to obtain one (link to OpenAI website).
	•	A "Save" button to persist the API key.
	•	Display current model: "gpt-4o-mini" (with option to change in the future).
	•	Storage:
	•	Store the API key securely (use localStorage or IndexedDB).
	•	Consider encryption for sensitive storage (for MVP, localStorage with a note about security considerations is acceptable).
	•	Validation:
	•	Validate API key format (starts with "sk-") on input.
	•	Optionally test the API key by making a simple API call when saved.
	•	Error handling:
	•	Show error if API key is invalid or missing when user tries to use chat.
	•	UI should be consistent with the rest of the app (use AppCard, AppButton, etc.).

	8.	Displaying Chats in Journal Views
	•	Update JournalView.vue to show chat indicators:
	•	For entries that have chat sessions, display a small badge or icon indicating "Chat available" or "X chat(s)".
	•	Optionally, allow users to view chat history from the journal list (could be a future enhancement, not required for this epic).
	•	Update JournalEditorView.vue (when editing an existing entry):
	•	If the entry has existing chat sessions, show a section or button to "View chat history" or "Continue chat".
	•	This allows users to revisit or continue previous conversations.

	9.	Testing Strategy
	•	Comprehensive test coverage for all new features:
	•	Unit tests (Vitest):
	•		- LLM service: API request construction, error handling, response parsing.
	•		- Chat store: State management, message sending, session saving.
	•		- Chat domain model: Data structure validation.
	•		- Journal store updates: Chat session persistence.
	•		- Database migration: Verify chatSessions field is added correctly to existing entries.
	•	Component tests:
	•		- JournalEditorView: Chat button visibility, dropdown menu, navigation to chat view.
	•		- ChatView: Message display, input handling, send message, save conversation.
	•		- ProfileView: API key input, validation, saving.
	•	Integration tests:
	•		- Full flow (save): Create entry → Click Chat → Select intention → Send messages → Save conversation → Verify chat is saved to entry.
	•		- Full flow (discard): Create entry → Click Chat → Select intention → Send messages → Leave without saving → Verify chat is NOT saved to entry.
	•		- Error scenarios: Invalid API key, network errors, API rate limits.
	•		- Database migration: Test that existing entries are migrated correctly.
	•	Edge cases:
	•		- Empty entry body (Chat button disabled).
	•		- Very long journal entries (ensure context is handled properly).
	•		- Multiple chat sessions per entry.
	•		- Custom intention with empty prompt.
	•		- Chat session with no messages (should not be saved).
	•		- Leaving without saving after having a conversation (confirmation dialog appears).
	•		- Leaving without saving when no messages sent (no confirmation needed).
	•		- Navigation away from chat view without saving (conversation is discarded).

⸻

3. Out of Scope (for this Epic)

	•	Streaming responses from the LLM (can be added in a future story).
	•	Chat history viewer/editor (beyond basic display in journal views).
	•	Exporting chats to external formats.
	•	Multiple LLM providers (only OpenAI for this epic).
	•	Advanced chat features (editing messages, deleting messages, regenerating responses).
	•	Chat templates or saved custom prompts.
	•	Analytics or insights derived from chat conversations.
	•	User authentication or cloud sync for API keys.
	•	Rate limiting UI (beyond basic error messages).
	•	Chat session search or filtering.

⸻

4. Success Criteria

The epic is considered complete when:

	1.	Users can add an OpenAI API key in the Profile view and it is saved securely.
	2.	In the Journal Editor, users see a "Chat" button with a dropdown menu of chat intentions.
	3.	Clicking "Chat" (after selecting an intention) saves the entry and navigates to the Chat interface.
	4.	The Chat interface displays the entry context and allows users to have a conversation with the LLM.
	5.	The LLM receives the entry's title, body, emotions, and tags as context and responds according to the selected intention.
	6.	Users can save chat conversations, which are stored and associated with the journal entry.
	7.	Users can leave the chat interface without saving, in which case the conversation is discarded and not added to the entry.
	8.	Chat sessions are persisted in the database only when explicitly saved by the user, and survive page refreshes.
	9.	Journal entries with chat sessions show appropriate indicators in the Journal view.
	10.	All new features have comprehensive test coverage (unit, component, and integration tests).
	11.	Error handling is robust (invalid API key, network errors, etc.) with user-friendly messages.
	12.	The UI is coherent with the existing journal editor design and follows Material Design principles.
	13.	Database migration successfully adds chatSessions field to existing entries without data loss.

⸻

5. Implementation Plan

The work should be divided into the following stories, implemented in this order:

	Story 1: API Key Management & LLM Service Foundation
	•	Add API key input field to ProfileView.
	•	Create storage mechanism for API key (IndexedDB).
	•	Create LLM service module with basic OpenAI API integration.
	•	Implement API key validation and error handling.
	•	Add unit tests for LLM service.
	•	Rationale: Foundation for all chat functionality. Must be in place before chat features can work.

	Story 2: Chat Data Model & Database Migration
	•	Define ChatSession domain model.
	•	Update JournalEntry interface to include chatSessions field.
	•	Create database migration to add chatSessions to existing entries.
	•	Update journalDexieRepository (if needed) to handle new field.
	•	Add unit tests for domain models and migration.
	•	Rationale: Data structure must be defined and migrated before storing chat data.

	Story 3: Chat Store & State Management
	•	Create useChatStore Pinia store.
	•	Implement chat session management (start, send message, save).
	•	Integrate with LLM service for API calls.
	•	Integrate with journal store for persisting chats to entries.
	•	Add unit tests for chat store.
	•	Rationale: Centralized state management for chat functionality. Needed before UI components.

	Story 4: System Prompts & Chat Intentions
	•	Define system prompts for each chat intention (Reflect, Help see differently, Help to be proactive, Thinking traps, Custom).
	•	Create a configuration module or constants file for system prompts.
	•	Add logic to construct initial context message from journal entry data.
	•	Add unit tests for prompt construction.
	•	Rationale: Core logic for how the LLM behaves. Should be defined before building the UI.

	Story 5: Journal Editor Chat Button & Navigation
	•	Add "Chat" button to JournalEditorView bottom action bar.
	•	Next to the "Chat" button implement dropdown menu for chat intention selection.
	•	Add validation (disable Chat button if entry body is empty).
	•	Implement save-and-navigate logic (save entry, then navigate to chat view).
	•	Add component tests for JournalEditorView chat button.
	•	Rationale: Entry point for chat feature. Users need a way to initiate chats.

	Story 6: Chat Interface View – Basic Structure
	•	Create ChatView.vue component.
	•	Add route /journal/:id/chat to router.
	•	Implement basic layout (top bar, entry context display, message area, input area).
	•	Style consistently with journal editor design.
	•	Add component tests for basic rendering.
	•	Rationale: Core UI structure must be in place before adding interactivity.

	Story 7: Chat Interface – Message Display & Sending
	•	Implement message display (user and AI messages with proper styling).
	•	Implement message input and send functionality.
	•	Integrate with chat store to send messages and receive responses.
	•	Add loading states and error handling in UI.
	•	Add component tests for message sending and display.
	•	Rationale: Core chat functionality. Users need to be able to communicate with the LLM.

	Story 8: Chat Interface – Save or Discard Conversation
	•	Implement "Save conversation" button:
	•		- Enabled after at least one message exchange.
	•		- Integrates with chat store to persist chat session to journal entry.
	•		- Shows success feedback (e.g., snackbar) when saved.
	•		- Navigates back to journal editor or journal list after saving.
	•	Implement "Leave without saving" or "End chat" button:
	•		- Shows confirmation dialog if there are unsaved messages (prevents accidental loss).
	•		- Calls chat store's discardChatSession() to clear unsaved conversation.
	•		- Navigates back to journal editor or journal list without saving.
	•		- No conversation data is persisted if user chooses to leave without saving.
	•	Add integration tests for both save and discard flows.
	•	Rationale: Users need control over whether to persist conversations. Some conversations may be exploratory and not worth saving.

	Story 9: Journal View Chat Indicators
	•	Update JournalView to display chat indicators for entries with chat sessions.
	•	Add ability to view/access chat history from journal list (optional: navigate to chat view or show in a modal).
	•	Update JournalEditorView to show existing chat sessions when editing an entry.
	•	Add component tests for chat indicators.
	•	Rationale: Users need to see and access their saved chat conversations.

	Story 10: Comprehensive Testing & Edge Cases
	•	Add integration tests for full chat flow (create entry → chat → save).
	•	Test error scenarios (invalid API key, network errors, rate limits).
	•	Test edge cases (empty body, long entries, multiple chats, custom prompts).
	•	Test database migration with existing data.
	•	Perform end-to-end testing and fix any bugs.
	•	Rationale: Ensure robustness and reliability of the feature before completion.

	Story 11: Polish & Documentation
	•	Review and refine UI/UX (spacing, colors, animations, accessibility).
	•	Add helpful tooltips or help text where needed.
	•	Ensure all error messages are user-friendly.
	•	Update documentation (if any) to include chat feature.
	•	Final code review and cleanup.
	•	Rationale: Ensure the feature is production-ready and user-friendly.

⸻

6. Technical Considerations

	•	API Key Security: For MVP, storing in localStorage is acceptable, but document security considerations. In production, consider encryption or secure storage solutions.
	•	Rate Limiting: OpenAI API has rate limits. Implement basic retry logic and user-friendly error messages. Consider adding rate limit indicators in a future story.
	•	Error Recovery: Implement robust error handling. If API call fails, allow users to retry without losing their conversation state.
	•	Accessibility: Ensure chat interface is accessible (keyboard navigation, screen reader support, ARIA labels).

⸻

7. Open Questions & Decisions Needed

	•	Should chat sessions be editable after saving? (Recommendation: No for MVP, can add in future.)
	•	Should users be able to delete individual chat sessions? (Recommendation: Yes, add delete functionality in Story 9 or 10.)
	•	What is the maximum number of messages per chat session? (Recommendation: No hard limit for MVP, but system prompts should encourage concise conversations.)
	•	Should custom prompts be saved for reuse? (Recommendation: No for MVP, can add in future.)
	•	How should very long journal entries be handled in the context? (Recommendation: Send full entry for MVP, truncate if needed based on token limits, can optimize in future.)

