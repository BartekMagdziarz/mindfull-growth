Story 10 – Comprehensive Testing & Edge Cases

Goal

Add comprehensive integration tests, edge case testing, and end-to-end validation for the entire chat feature. This story ensures the robustness and reliability of all chat functionality by testing full user flows, error scenarios, edge cases, and database migrations. Any bugs discovered during testing are fixed to ensure the feature is production-ready.

⸻

Scope

	1.	Full Chat Flow Integration Tests
	•	Create comprehensive integration tests that cover the complete user journey:
	•	Test: Create Entry → Start Chat → Send Messages → Save Conversation
	•	Steps:
	•	Create a new journal entry with title, body, emotions, and tags.
	•	Save the entry.
	•	Navigate to journal editor.
	•	Click Chat button and select an intention (test each intention type).
	•	Verify navigation to chat view.
	•	Verify entry context is displayed correctly (title, emotions, tags, body preview).
	•	Send a user message.
	•	Verify AI response is received and displayed.
	•	Send additional messages (test multi-turn conversation).
	•	Click "Save conversation" button.
	•	Verify success feedback is shown.
	•	Verify navigation back to journal editor.
	•	Verify chat session is saved to the entry (check entry.chatSessions array).
	•	Refresh the page and verify chat session persists.
	•	Test: Create Entry → Start Chat → Send Messages → Discard Conversation
	•	Steps:
	•	Create a new journal entry.
	•	Start a chat session and send messages.
	•	Click "Leave without saving" button.
	•	Verify confirmation dialog appears (if messages exist).
	•	Confirm discard.
	•	Verify navigation occurs.
	•	Verify chat session is NOT saved to the entry.
	•	Refresh the page and verify chat session is not present.
	•	Test: View Existing Chat Session
	•	Steps:
	•	Create an entry with a saved chat session.
	•	Navigate to journal editor.
	•	Click on existing chat session (from Story 9).
	•	Verify chat view loads with the correct conversation.
	•	Verify all messages are displayed correctly.
	•	Test: Multiple Chat Sessions Per Entry
	•	Steps:
	•	Create a journal entry.
	•	Start and save a chat session with "Reflect" intention.
	•	Start and save another chat session with "Help see differently" intention.
	•	Start and save a third chat session with "Custom" intention and custom prompt.
	•	Verify all three chat sessions are saved to the entry.
	•	Verify chat indicator shows correct count (3 chats).
	•	Verify each chat session can be viewed independently.
	•	Test: Chat Button Validation
	•	Steps:
	•	Create a new journal entry with empty body.
	•	Verify Chat button is disabled.
	•	Add content to entry body.
	•	Verify Chat button is enabled.
	•	Test: Chat with Different Intentions
	•	Test each chat intention separately:
	•	"Reflect" intention – verify appropriate system prompt is used.
	•	"Help see differently" intention – verify appropriate system prompt is used.
	•	"Help to be proactive" intention – verify appropriate system prompt is used.
	•	"Thinking traps" intention – verify appropriate system prompt is used.
	•	"Custom" intention with custom prompt – verify custom prompt is used.
	•	"Custom" intention with empty prompt – verify error handling.

	2.	Error Scenario Testing
	•	Test: Invalid API Key
	•	Steps:
	•	Set an invalid API key in user settings.
	•	Start a chat session.
	•	Send a message.
	•	Verify error message is displayed: "Invalid API key. Please check your API key in Profile settings."
	•	Verify user can retry after fixing the API key.
	•	Test: Missing API Key
	•	Steps:
	•	Remove API key from user settings.
	•	Start a chat session.
	•	Send a message.
	•	Verify error message is displayed: "OpenAI API key is not configured. Please add your API key in Profile settings."
	•	Verify error message includes link or guidance to Profile view.
	•	Test: Network Errors
	•	Steps:
	•	Mock network failure (disconnect or simulate network error).
	•	Start a chat session and send a message.
	•	Verify error message is displayed: "Network error. Please check your connection and try again."
	•	Verify user can retry the message.
	•	Verify conversation state is preserved (user message is not lost).
	•	Test: API Rate Limiting
	•	Steps:
	•	Mock API to return 429 Too Many Requests response.
	•	Send a message.
	•	Verify error message is displayed: "Rate limit exceeded. Please try again in a moment."
	•	Verify user can retry after waiting.
	•	Test: API Service Errors
	•	Steps:
	•	Mock API to return various error responses (500, 503, etc.).
	•	Send a message.
	•	Verify user-friendly error message is displayed.
	•	Verify technical details are logged to console (not shown to user).
	•	Test: Entry Deletion During Chat
	•	Steps:
	•	Start a chat session for an entry.
	•	Delete the entry (from another view or in a separate test).
	•	Try to save the chat session.
	•	Verify error message is displayed: "Entry not found. Cannot save conversation."
	•	Verify user can navigate away gracefully.
	•	Test: Database Errors During Save
	•	Steps:
	•	Mock database to fail during save operation.
	•	Start a chat session and send messages.
	•	Click "Save conversation".
	•	Verify error message is displayed: "Database error. Please try again."
	•	Verify user can retry saving.
	•	Verify conversation is not lost (still in memory).

	3.	Edge Case Testing
	•	Test: Empty Entry Body
	•	Steps:
	•	Create entry with empty body.
	•	Verify Chat button is disabled.
	•	Add minimal content (single character).
	•	Verify Chat button is enabled.
	•	Start chat and verify it works.
	•	Test: Very Long Journal Entries
	•	Steps:
	•	Create entry with very long body (10,000+ characters).
	•	Start a chat session.
	•	Verify entry context is included correctly (full entry or truncated appropriately).
	•	Verify chat functionality works with long context.
	•	Verify LLM receives the context (check API request payload in tests).
	•	Test: Entry with Many Emotions and Tags
	•	Steps:
	•	Create entry with maximum emotions (all available emotions).
	•	Create entry with many people tags and context tags.
	•	Start a chat session.
	•	Verify all emotions and tags are included in context.
	•	Verify context message is formatted correctly.
	•	Test: Entry with Special Characters
	•	Steps:
	•	Create entry with special characters in title and body (emojis, unicode, quotes, newlines).
	•	Start a chat session.
	•	Verify special characters are handled correctly.
	•	Verify context message is properly formatted.
	•	Verify messages with special characters work correctly.
	•	Test: Custom Intention with Empty Prompt
	•	Steps:
	•	Start chat with "Custom" intention.
	•	Leave custom prompt empty or provide only whitespace.
	•	Verify validation error is shown (or handle gracefully).
	•	Test: Custom Intention with Very Long Prompt
	•	Steps:
	•	Start chat with "Custom" intention.
	•	Provide a very long custom prompt (1000+ characters).
	•	Verify it's handled correctly.
	•	Verify system prompt includes the custom prompt.
	•	Test: Chat Session with No Messages
	•	Steps:
	•	Start a chat session.
	•	Don't send any messages.
	•	Try to save the conversation.
	•	Verify save button is disabled or shows appropriate error.
	•	Test: Chat Session with Only User Messages (No AI Response)
	•	Steps:
	•	Start a chat session.
	•	Send a user message.
	•	Mock API to fail before response.
	•	Try to save the conversation.
	•	Verify save button is disabled or shows appropriate error.
	•	Test: Very Long Chat Conversations
	•	Steps:
	•	Start a chat session.
	•	Send many messages (20+ exchanges).
	•	Verify all messages are displayed correctly.
	•	Verify conversation history is maintained correctly.
	•	Verify LLM receives full conversation history.
	•	Test: Rapid Message Sending
	•	Steps:
	•	Start a chat session.
	•	Send multiple messages rapidly (before AI responds).
	•	Verify messages are queued or handled appropriately.
	•	Verify loading states are managed correctly.
	•	Test: Navigation During Active Chat
	•	Steps:
	•	Start a chat session and send messages.
	•	Use browser back button.
	•	Verify confirmation dialog appears (if unsaved messages).
	•	Confirm discard.
	•	Verify conversation is not saved.
	•	Test: Page Refresh During Chat
	•	Steps:
	•	Start a chat session and send messages (unsaved).
	•	Refresh the page.
	•	Verify conversation is lost (expected behavior for unsaved chats).
	•	Test: Multiple Tabs/Windows
	•	Steps:
	•	Open app in multiple tabs.
	•	Start a chat session in one tab.
	•	Verify other tabs don't interfere.
	•	Save chat session in one tab.
	•	Verify other tabs see the update (if using shared storage).

	4.	Database Migration Testing
	•	Test: Migration from Version 4 to Version 5
	•	Steps:
	•	Create a test database with version 4 schema (no chatSessions field).
	•	Add multiple journal entries with various data (some with emotions, tags, etc.).
	•	Run the app to trigger migration to version 5.
	•	Verify all entries are migrated correctly:
	•	All entries have chatSessions field (as empty array).
	•	No data loss (all other fields are preserved).
	•	Entries can be retrieved and updated correctly.
	•	Test: Migration Idempotency
	•	Steps:
	•	Run migration from version 4 to version 5.
	•	Run migration again (simulate app restart).
	•	Verify no errors occur.
	•	Verify no duplicate data is created.
	•	Verify entries still have correct chatSessions field.
	•	Test: Migration with Existing chatSessions
	•	Steps:
	•	Create a test database with version 4 schema.
	•	Manually add some entries with chatSessions field (simulating partial migration).
	•	Run migration.
	•	Verify existing chatSessions are preserved.
	•	Verify entries without chatSessions get empty array.
	•	Test: Migration with Empty Database
	•	Steps:
	•	Create an empty database with version 4 schema.
	•	Run migration.
	•	Verify no errors occur.
	•	Verify new entries can be created with chatSessions.
	•	Test: Migration with Large Database
	•	Steps:
	•	Create a test database with version 4 schema.
	•	Add many entries (100+ entries).
	•	Run migration.
	•	Verify all entries are migrated correctly.
	•	Verify migration completes in reasonable time.
	•	Test: Migration Error Recovery
	•	Steps:
	•	Simulate migration failure (corrupt entry, database error).
	•	Run migration.
	•	Verify app still starts (migration errors don't crash app).
	•	Verify error is logged appropriately.
	•	Verify successfully migrated entries are correct.
	•	Verify failed entries can be migrated on next attempt.

	5.	Component Integration Testing
	•	Test: JournalEditorView Chat Button Integration
	•	Steps:
	•	Test Chat button visibility and state (enabled/disabled).
	•	Test dropdown menu for intention selection.
	•	Test navigation to chat view after selecting intention.
	•	Test that entry is saved before navigation.
	•	Test: ChatView Component Integration
	•	Steps:
	•	Test entry context display (title, emotions, tags, body preview).
	•	Test message display (user and AI messages).
	•	Test message input and sending.
	•	Test loading states during API calls.
	•	Test error display.
	•	Test save and discard buttons.
	•	Test navigation guards.
	•	Test: JournalView Chat Indicators Integration
	•	Steps:
	•	Test chat indicator badge display for entries with chats.
	•	Test badge count accuracy.
	•	Test clicking badge opens chat history.
	•	Test chat history modal/list display.
	•	Test: ProfileView API Key Management Integration
	•	Steps:
	•	Test API key input and validation.
	•	Test saving API key.
	•	Test error messages for invalid keys.
	•	Test that saved API key is used by LLM service.

	6.	End-to-End Testing
	•	Perform manual end-to-end testing of the complete feature:
	•	Test the full user journey from start to finish:
	•	Open the app.
	•	Add API key in Profile view.
	•	Create a journal entry.
	•	Start a chat session with each intention type.
	•	Have conversations with the AI.
	•	Save conversations.
	•	View saved conversations.
	•	Delete chat sessions.
	•	Test with real OpenAI API (if test API key is available).
	•	Test error scenarios manually:
	•	Invalid API key.
	•	Network disconnection.
	•	API rate limiting.
	•	Test edge cases manually:
	•	Very long entries.
	•	Many emotions and tags.
	•	Special characters.
	•	Multiple chat sessions.
	•	Test database migration manually:
	•	Create entries in an old version of the app.
	•	Upgrade to new version.
	•	Verify migration works correctly.
	•	Test accessibility:
	•	Keyboard navigation.
	•	Screen reader compatibility.
	•	Focus management.
	•	Test performance:
	•	App startup time with many entries.
	•	Chat view loading time.
	•	Message sending response time.
	•	Database operations performance.

	7.	Bug Fixes & Refinements
	•	Document all bugs discovered during testing:
	•	Create a list of bugs with descriptions, steps to reproduce, and severity.
	•	Prioritize bugs (critical, high, medium, low).
	•	Fix critical and high-priority bugs:
	•	Fix data loss issues.
	•	Fix crashes or app-breaking bugs.
	•	Fix incorrect behavior that affects core functionality.
	•	Fix medium-priority bugs if time permits:
	•	UI/UX issues.
	•	Minor error handling improvements.
	•	Performance optimizations.
	•	Document low-priority bugs for future fixes:
	•	Nice-to-have improvements.
	•	Edge cases that are unlikely to occur in normal usage.
	•	Code cleanup and refactoring:
	•	Remove unused code.
	•	Improve code comments and documentation.
	•	Refactor complex functions for better maintainability.
	•	Ensure consistent error handling patterns.
	•	Ensure consistent naming conventions.

	8.	Test Coverage Analysis
	•	Analyze test coverage for all chat-related code:
	•	Run test coverage reports (using Vitest coverage).
	•	Identify areas with low coverage.
	•	Add tests for uncovered code paths.
	•	Target coverage goals:
	•	Domain models: 100% coverage.
	•	Services: 90%+ coverage (LLM service, chat prompts).
	•	Stores: 90%+ coverage (chat store).
	•	Repositories: 90%+ coverage (migration logic).
	•	Components: 80%+ coverage (critical paths).
	•	Integration tests: Cover all major user flows.
	•	Document coverage gaps and rationale for any areas below targets.

	9.	Performance Testing
	•	Test performance of chat-related operations:
	•	Database operations:
	•	Test saving chat sessions with many entries.
	•	Test loading entries with many chat sessions.
	•	Test migration performance with large databases.
	•	API operations:
	•	Test message sending response times.
	•	Test handling of slow API responses.
	•	Test concurrent message sending (if applicable).
	•	UI rendering:
	•	Test chat view with many messages.
	•	Test journal view with many entries and chat indicators.
	•	Test scrolling performance in chat view.
	•	Memory usage:
	•	Test memory usage with many chat sessions.
	•	Test for memory leaks (long-running sessions).
	•	Identify and fix performance bottlenecks:
	•	Optimize slow database queries.
	•	Optimize UI rendering (virtual scrolling if needed).
	•	Optimize API request construction.

	10.	Accessibility Testing
	•	Test accessibility of chat-related UI:
	•	Keyboard navigation:
	•	Test navigating chat interface with keyboard only.
	•	Test focus management in modals and dialogs.
	•	Test tab order in forms and buttons.
	•	Screen reader compatibility:
	•	Test with screen reader (NVDA, JAWS, or VoiceOver).
	•	Verify all UI elements have proper ARIA labels.
	•	Verify error messages are announced.
	•	Verify loading states are announced.
	•	Color contrast:
	•	Verify text meets WCAG contrast requirements.
	•	Verify UI elements are distinguishable without color alone.
	•	Fix accessibility issues:
	•	Add missing ARIA labels.
	•	Improve focus management.
	•	Fix color contrast issues.
	•	Ensure all interactive elements are keyboard accessible.

⸻

Acceptance Criteria

	•	Integration Tests:
	•	All full chat flow integration tests pass (create → chat → save, create → chat → discard, view existing, multiple sessions, etc.).
	•	All error scenario tests pass (invalid API key, missing API key, network errors, rate limits, etc.).
	•	All edge case tests pass (empty body, long entries, many emotions/tags, special characters, etc.).
	•	All database migration tests pass (version 4 to 5, idempotency, existing chats, empty database, large database, error recovery).
	•	All component integration tests pass (JournalEditorView, ChatView, JournalView, ProfileView).
	•	Test Coverage:
	•	Domain models have 100% test coverage.
	•	Services have 90%+ test coverage.
	•	Stores have 90%+ test coverage.
	•	Repositories have 90%+ test coverage (especially migration logic).
	•	Components have 80%+ test coverage for critical paths.
	•	Integration tests cover all major user flows.
	•	Bug Fixes:
	•	All critical bugs are fixed.
	•	All high-priority bugs are fixed.
	•	Medium-priority bugs are fixed (or documented for future fixes).
	•	Code is cleaned up and refactored where needed.
	•	End-to-End Testing:
	•	Manual end-to-end testing is completed.
	•	All user journeys work correctly.
	•	Error scenarios are handled gracefully.
	•	Edge cases are handled appropriately.
	•	Database migration works correctly with real data.
	•	Performance:
	•	App performance is acceptable (no significant slowdowns).
	•	Database operations are performant.
	•	API operations are handled efficiently.
	•	UI rendering is smooth.
	•	No memory leaks are detected.
	•	Accessibility:
	•	All accessibility issues are fixed.
	•	Keyboard navigation works correctly.
	•	Screen reader compatibility is verified.
	•	Color contrast meets WCAG requirements.
	•	The app can be started and:
	•	All tests pass (unit, component, integration).
	•	No critical bugs are present.
	•	Feature is production-ready.
	•	Linting and TypeScript checks pass with no errors.
	•	Test coverage meets targets.

⸻

Out of Scope

	•	Adding new features (this story is focused on testing and fixing existing features).
	•	Performance optimizations beyond fixing critical bottlenecks (major optimizations can be done in future stories).
	•	Accessibility enhancements beyond fixing critical issues (nice-to-have improvements can be done later).
	•	Writing documentation for end users (technical documentation is sufficient for this story).
	•	Adding analytics or monitoring (can be added in future stories).
	•	Streaming responses from LLM (out of scope for this epic).
	•	Advanced error recovery beyond basic retry (sufficient for MVP).

⸻

Technical Considerations

	•	Test Organization:
	•	Organize integration tests by feature area (chat flow, error scenarios, edge cases, migration).
	•	Use descriptive test names that explain what is being tested.
	•	Group related tests using `describe` blocks.
	•	Use test utilities and helpers to reduce code duplication.
	•	Mocking Strategy:
	•	Mock external dependencies (OpenAI API, IndexedDB) in unit and component tests.
	•	Use real implementations in integration tests (or mocked implementations that behave like real ones).
	•	Mock network conditions for error scenario testing.
	•	Mock time for testing timestamps and date formatting.
	•	Test Data Management:
	•	Use isolated test databases for each test (don't share state between tests).
	•	Clean up test data after each test.
	•	Use factories or builders to create test data consistently.
	•	Performance Testing:
	•	Use performance profiling tools to identify bottlenecks.
	•	Test with realistic data sizes (not just small test cases).
	•	Measure and document performance metrics.
	•	Accessibility Testing:
	•	Use automated accessibility testing tools (axe-core, Lighthouse) in addition to manual testing.
	•	Test with actual screen readers when possible.
	•	Follow WCAG 2.1 Level AA guidelines as a minimum.
	•	Bug Tracking:
	•	Document bugs clearly with steps to reproduce.
	•	Prioritize bugs based on severity and impact.
	•	Fix bugs systematically, starting with critical issues.
	•	Test fixes thoroughly before considering them complete.

⸻

Implementation Notes

	•	Integration Test Structure:
	•	Create test files in `src/__tests__/integration/` directory:
	•	`chat-full-flow.spec.ts` – full chat flow tests.
	•	`chat-error-scenarios.spec.ts` – error scenario tests.
	•	`chat-edge-cases.spec.ts` – edge case tests.
	•	`chat-migration.spec.ts` – database migration tests.
	•	Use test utilities from `src/__tests__/integration/testUtils.ts` if available.
	•	Example integration test structure:
	•	```typescript
	•	import { describe, it, expect, beforeEach, afterEach } from 'vitest'
	•	import { useChatStore } from '@/stores/chat.store'
	•	import { useJournalStore } from '@/stores/journal.store'
	•	
	•	describe('Chat Full Flow', () => {
	•	  beforeEach(() => {
	•	    // Setup test database
	•	   })
	•	  
	•	  afterEach(() => {
	•	    // Cleanup test data
	•	   })
	•	  
	•	  it('should create entry, start chat, send messages, and save conversation', async () => {
	•	    // Test implementation
	•	   })
	•	})
	•	```
	•	Error Scenario Testing:
	•	Mock OpenAI API to return various error responses.
	•	Use `vi.mock` to mock the LLM service.
	•	Test error messages are user-friendly and actionable.
	•	Example:
	•	```typescript
	•	vi.mock('@/services/llmService', () => ({
	•	  sendMessage: vi.fn().mockRejectedValue(new Error('Invalid API key'))
	•	}))
	•	```
	•	Edge Case Testing:
	•	Create test data factories for various edge cases.
	•	Test with realistic but extreme data (very long strings, many items, etc.).
	•	Verify system handles edge cases gracefully (no crashes, appropriate error messages).
	•	Migration Testing:
	•	Create test utilities to set up databases with specific versions.
	•	Test migration logic in isolation.
	•	Verify data integrity after migration.
	•	Example:
	•	```typescript
	•	async function createV4Database() {
	•	  // Create database with version 4 schema
	•	}
	•	
	•	it('should migrate entries from version 4 to version 5', async () => {
	•	  const db = await createV4Database()
	•	  // Add test entries
	•	  // Run migration
	•	  // Verify migration
	•	})
	•	```
	•	Performance Testing:
	•	Use Vitest's performance testing capabilities or add performance benchmarks.
	•	Measure and document metrics (response times, memory usage, etc.).
	•	Example:
	•	```typescript
	•	it('should save chat session quickly', async () => {
	•	  const start = performance.now()
	•	  await chatStore.saveChatSession(entryId, session)
	•	  const duration = performance.now() - start
	•	  expect(duration).toBeLessThan(100) // Should complete in < 100ms
	•	})
	•	```
	•	Accessibility Testing:
	•	Use automated tools like `@axe-core/vitest` for accessibility testing.
	•	Add accessibility tests to component test suites.
	•	Example:
	•	```typescript
	•	import { axe } from 'vitest-axe'
	•	
	•	it('should have no accessibility violations', async () => {
	•	  const { container } = render(ChatView)
	•	  const results = await axe(container)
	•	  expect(results).toHaveNoViolations()
	•	})
	•	```
	•	Bug Tracking:
	•	Create a simple bug tracking document or use GitHub issues.
	•	Document each bug with:
	•	Description
	•	Steps to reproduce
	•	Expected behavior
	•	Actual behavior
	•	Severity (critical, high, medium, low)
	•	Status (open, in progress, fixed, verified)
	•	File Organization:
	•	Integration tests: `src/__tests__/integration/`
	•	Performance tests: `src/__tests__/performance/` (if separate from integration tests)
	•	Accessibility tests: Include in component test files or separate `accessibility/` directory
	•	Test utilities: `src/__tests__/utils/` or `src/__tests__/integration/testUtils.ts`
	•	Bug documentation: Create `BUGS.md` or use project management tool
	•	Dependencies:
	•	Ensure test dependencies are available:
	•	`@testing-library/vue` for component testing
	•	`@axe-core/vitest` for accessibility testing (optional)
	•	Performance testing tools (if needed)
	•	Testing:
	•	Run all tests before considering the story complete.
	•	Ensure tests are fast and reliable (no flaky tests).
	•	Fix any failing tests.
	•	Update tests if implementation changes.
	•	Document any test coverage gaps and rationale.

