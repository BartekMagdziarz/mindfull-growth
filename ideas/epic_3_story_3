Story 3 – Chat Store & State Management

Goal

Create a centralized Pinia store for managing chat session state and interactions. This store will handle starting chat sessions, sending messages to the LLM, managing conversation state (both saved and unsaved), and persisting chat sessions to journal entries. This story establishes the state management layer that the chat UI components (to be built in later stories) will depend on.

⸻

Scope

	1.	Chat Store Structure
	•	Create a new Pinia store: `src/stores/chat.store.ts`
	•	Store name: `useChatStore`
	•	State properties:
	•	`currentChatSession: ChatSession | null` – the active/current chat session being worked on
	•	`isLoading: boolean` – indicates if an LLM API call is in progress
	•	`error: string | null` – stores any error messages from API calls or operations
	•	`journalEntryId: string | null` – the ID of the journal entry associated with the current chat session
	•	State should be reactive and properly typed with TypeScript
	•	The store should be a singleton (created once and shared across the app)

	2.	Chat Session Management Actions
	•	Implement the following actions in the store:
	•	`startChatSession(entryId: string, intention: string, customPrompt?: string): Promise<void>`
	•	Initializes a new chat session for the given journal entry
	•	Creates a new ChatSession object with:
	•	Generated UUID for the session ID
	•	The provided journalEntryId
	•	The provided intention (must be a valid ChatIntention type)
	•	The optional customPrompt (only if intention is "custom")
	•	Current timestamp for createdAt
	•	Empty messages array
	•	Sets the currentChatSession state to this new session
	•	Sets journalEntryId state
	•	Clears any previous error state
	•	Does NOT persist the session yet (it's only in memory until explicitly saved)
	•	Uses the factory functions from domain/chatSession.ts if they exist (from Story 2)
	•	`sendMessage(message: string): Promise<void>`
	•	Adds the user's message to the current chat session's messages array
	•	Creates a ChatMessage with role "user", the message content, and current timestamp
	•	Calls the LLM service to get an AI response
	•	Constructs the conversation history for the LLM:
	•	Includes the system prompt (based on the chat intention, to be implemented in Story 4)
	•	Includes an initial context message with the journal entry data (title, body, emotions, tags)
	•	Includes all previous messages in the conversation
	•	Includes the new user message
	•	Sends the request to the LLM service
	•	Handles loading state (set isLoading to true before API call, false after)
	•	On success:
	•	Adds the AI's response as a ChatMessage with role "assistant" to the messages array
	•	Clears any error state
	•	On error:
	•	Sets the error state with a user-friendly error message
	•	Does NOT add the user message if the API call fails (to keep conversation state consistent)
	•	Allows retry of failed messages
	•	`saveChatSession(): Promise<void>`
	•	Validates that there is a current chat session
	•	Validates that the chat session has at least one message exchange (user message + assistant response)
	•	Retrieves the journal entry using the journal store
	•	Adds the current chat session to the entry's chatSessions array
	•	If chatSessions is undefined or null, initializes it as an empty array first
	•	Updates the journal entry using the journal store's update method
	•	Handles errors gracefully (e.g., entry not found, save failure)
	•	On success:
	•	Clears the current chat session state (sets to null)
	•	Clears the journalEntryId state
	•	Shows success feedback (this will be handled by the UI component, but the store can emit an event or return a success indicator)
	•	`discardChatSession(): void`
	•	Clears the current chat session state (sets to null)
	•	Clears the journalEntryId state
	•	Clears any error state
	•	Does NOT persist anything to the database
	•	The conversation is lost and cannot be recovered
	•	`loadChatSessionsForEntry(entryId: string): Promise<ChatSession[]>`
	•	Retrieves the journal entry using the journal store
	•	Returns the entry's chatSessions array (or empty array if none exist)
	•	Handles errors gracefully (e.g., entry not found)
	•	This action is useful for displaying chat history in the UI (to be used in later stories)

	3.	LLM Service Integration
	•	Import and use the LLM service from Story 1 (`src/services/llmService.ts`)
	•	In the `sendMessage` action:
	•	Call `llmService.sendMessage(messages, systemPrompt)` with:
	•	messages: Array of conversation messages (system, context, and conversation history)
	•	systemPrompt: The system prompt based on the chat intention (to be implemented in Story 4, for now use a placeholder or basic prompt)
	•	Handle the Promise returned by the LLM service
	•	Handle errors from the LLM service:
	•	Missing API key: Set error message "OpenAI API key is not configured. Please add your API key in Profile settings."
	•	Invalid API key: Set error message "Invalid API key. Please check your API key in Profile settings."
	•	Rate limit: Set error message "Rate limit exceeded. Please try again in a moment."
	•	Network errors: Set error message "Network error. Please check your connection and try again."
	•	Other API errors: Set a generic user-friendly error message
	•	Log detailed errors to console for debugging (without exposing sensitive data)
	•	Note: System prompts will be fully implemented in Story 4. For Story 3, use a basic placeholder system prompt or import a temporary constant.

	4.	Journal Store Integration
	•	Import and use the journal store (likely `useJournalStore` from `src/stores/journal.store.ts`)
	•	In the `saveChatSession` action:
	•	Use the journal store to retrieve the entry: `journalStore.getEntryById(entryId)`
	•	Use the journal store to update the entry: `journalStore.updateEntry(updatedEntry)`
	•	Ensure the journal store handles the chatSessions field correctly (should be automatic since it's part of the JournalEntry interface)
	•	Handle cases where the entry might not exist (should not happen in normal flow, but handle gracefully)
	•	Note: If the journal store doesn't have a `getEntryById` method, use the journal repository directly or add the method to the store

	5.	Journal Entry Context Construction
	•	In the `sendMessage` action, construct the initial context message for the LLM:
	•	Retrieve the journal entry using the journal store
	•	Extract relevant information:
	•	Entry title (or "Untitled entry" if empty)
	•	Entry body (full text)
	•	Selected emotions (names of emotions, not just IDs)
	•	Selected tags (people and context tags, names not just IDs)
	•	Format this information into a context message string
	•	Example format:
	•	"Journal Entry Context:
	•	Title: [title]
	•	Emotions: [emotion1, emotion2, ...]
	•	Tags: [tag1, tag2, ...]
	•	Content: [body text]"
	•	Add this context message as the first user message in the conversation (or as a system message, depending on the LLM service API structure)
	•	This context message should only be sent once at the start of the conversation, not with every message
	•	Store a flag or check if this is the first message to determine when to include context

	6.	State Management Best Practices
	•	Use Pinia's reactive state properly
	•	Use actions for all state mutations (never mutate state directly from outside the store)
	•	Use getters for computed/derived state if needed (e.g., `hasUnsavedMessages: boolean` getter)
	•	Consider adding a getter:
	•	`hasUnsavedMessages: boolean` – returns true if currentChatSession exists and has at least one message
	•	This getter can be used by UI components to determine if a confirmation dialog should be shown when leaving
	•	Reset state appropriately:
	•	When starting a new session, clear previous state
	•	When saving or discarding, clear the current session
	•	Handle edge cases:
	•	What happens if sendMessage is called without a current session? (Should throw error or set error state)
	•	What happens if saveChatSession is called without a current session? (Should throw error or set error state)
	•	What happens if saveChatSession is called with an empty conversation? (Should throw error or set error state)

	7.	Error Handling & User Feedback
	•	All errors should be stored in the error state as user-friendly messages
	•	Technical error details should be logged to console for debugging
	•	Error messages should be actionable (tell the user what they can do to fix the issue)
	•	Consider error recovery:
	•	If an API call fails, the user should be able to retry without losing their conversation state
	•	The store should maintain the conversation state even if an API call fails (the user message should be added only after successful API response, or added immediately and marked as "pending" – choose one approach and be consistent)
	•	Loading states should be properly managed:
	•	isLoading should be true only during active API calls
	•	isLoading should be false when API call completes (success or error)

	8.	Unit Tests
	•	Add comprehensive unit tests (Vitest) for the chat store:
	•	Store Initialization:
	•	Test that the store is created with correct initial state
	•	Test that all state properties are properly initialized
	•	startChatSession Action:
	•	Test creating a new chat session with valid parameters
	•	Test creating a chat session with custom intention and customPrompt
	•	Test that the session is created with correct structure (id, journalEntryId, intention, etc.)
	•	Test that previous session state is cleared when starting a new session
	•	Test error handling for invalid parameters
	•	sendMessage Action:
	•	Test sending a message when no session exists (should error)
	•	Test sending a message successfully:
	•	Mock the LLM service to return a successful response
	•	Verify that user message is added to messages array
	•	Verify that assistant response is added to messages array
	•	Verify that loading state is managed correctly
	•	Test error handling:
	•	Mock LLM service to throw various errors (missing API key, invalid key, rate limit, network error)
	•	Verify that error state is set correctly
	•	Verify that user message is NOT added if API call fails (or verify the chosen approach)
	•	Test that conversation context is included in the first message
	•	Test that subsequent messages don't re-include the context
	•	Test retry functionality (if implemented)
	•	saveChatSession Action:
	•	Test saving a chat session with valid conversation (at least one exchange)
	•	Mock the journal store to verify that updateEntry is called with correct data
	•	Verify that the chat session is added to the entry's chatSessions array
	•	Verify that state is cleared after successful save
	•	Test error handling:
	•	Test saving when no session exists (should error)
	•	Test saving when session has no messages (should error)
	•	Test saving when entry doesn't exist (should error)
	•	Test saving when journal store update fails (should error)
	•	discardChatSession Action:
	•	Test discarding a chat session clears all state
	•	Test discarding when no session exists (should not error, just clear state)
	•	loadChatSessionsForEntry Action:
	•	Test loading chat sessions for an entry that has chats
	•	Test loading chat sessions for an entry with no chats (returns empty array)
	•	Test loading chat sessions for non-existent entry (should error gracefully)
	•	Getters:
	•	Test hasUnsavedMessages getter (if implemented)
	•	Integration with Other Services:
	•	Test that LLM service is called correctly with proper parameters
	•	Test that journal store is called correctly for retrieving and updating entries
	•	Use mocking for:
	•	LLM service (mock the sendMessage function)
	•	Journal store (mock getEntryById and updateEntry methods)
	•	UUID generation (if using a library, mock it for consistent test IDs)
	•	Follow existing test patterns in the codebase
	•	Use descriptive test names
	•	Test both happy paths and error scenarios

⸻

Acceptance Criteria

	•	Chat Store:
	•	`src/stores/chat.store.ts` file exists with useChatStore defined.
	•	Store has all required state properties: currentChatSession, isLoading, error, journalEntryId.
	•	Store has all required actions: startChatSession, sendMessage, saveChatSession, discardChatSession, loadChatSessionsForEntry.
	•	State is properly typed with TypeScript.
	•	Actions:
	•	startChatSession creates a new chat session with correct structure and sets it as current session.
	•	sendMessage adds user message, calls LLM service, adds assistant response, and manages loading/error states correctly.
	•	saveChatSession validates session, adds it to journal entry's chatSessions array, and persists to database.
	•	discardChatSession clears current session state without persisting.
	•	loadChatSessionsForEntry retrieves and returns chat sessions for a given entry.
	•	LLM Service Integration:
	•	Store correctly imports and uses the LLM service from Story 1.
	•	Store constructs conversation messages correctly (system prompt, context, history).
	•	Store handles all LLM service errors gracefully with user-friendly messages.
	•	Loading states are managed correctly (isLoading true during API calls, false after).
	•	Journal Store Integration:
	•	Store correctly imports and uses the journal store.
	•	Store retrieves journal entries correctly.
	•	Store updates journal entries with chat sessions correctly.
	•	Journal Entry Context:
	•	Initial context message includes entry title, body, emotions, and tags.
	•	Context is included only in the first message, not in subsequent messages.
	•	Context is formatted in a readable way for the LLM.
	•	Error Handling:
	•	All errors are stored as user-friendly messages in error state.
	•	Technical errors are logged to console for debugging.
	•	Error messages are actionable.
	•	Edge Cases:
	•	Calling sendMessage without a session shows appropriate error.
	•	Calling saveChatSession without a session shows appropriate error.
	•	Calling saveChatSession with empty conversation shows appropriate error.
	•	State is properly reset when starting new sessions or saving/discarding.
	•	The app can be started and:
	•	Chat store can be imported and used.
	•	Store actions can be called (though they may error if dependencies aren't fully set up yet).
	•	Unit tests for chat store pass.
	•	Linting and TypeScript checks pass with no new errors.

⸻

Out of Scope

	•	Chat UI components (will be implemented in Stories 6 and 7).
	•	System prompts implementation (will be implemented in Story 4; use placeholder for now).
	•	Streaming responses from LLM (will be added in a future story).
	•	Chat session editing or deletion (will be implemented in later stories).
	•	Chat history viewer UI (will be implemented in Story 9).
	•	Advanced error recovery UI (basic error handling is sufficient for this story).
	•	Message editing or deletion within a conversation (out of scope for this epic).
	•	Retry logic with exponential backoff (basic retry is sufficient for MVP).
	•	Conversation state persistence across page refreshes for unsaved chats (unsaved chats are intentionally lost on refresh, as per epic requirements).

⸻

Technical Considerations

	•	State Management Pattern:
	•	Follow Pinia best practices: use actions for mutations, getters for computed state.
	•	Keep the store focused on chat-related state only. Don't mix in journal entry editing state.
	•	Consider using Pinia's storeToRefs helper in components (to be used in later stories) to maintain reactivity.
	•	Conversation Context:
	•	The initial context message should be constructed once when the first user message is sent.
	•	Consider storing a flag (e.g., `contextSent: boolean`) in the chat session or checking if messages array is empty to determine when to include context.
	•	Alternatively, always include context in the first message construction, but don't add it as a separate message to the array.
	•	Error Recovery:
	•	Decide on approach for failed API calls:
	•	Option A: Don't add user message until API succeeds (keeps conversation state consistent, but user might lose their message if they navigate away)
	•	Option B: Add user message immediately, mark as "pending", then add assistant response when API succeeds (preserves user input, but requires handling "pending" state)
	•	Recommendation: Use Option A for MVP (simpler, and user can retry if needed). Document this decision.
	•	UUID Generation:
	•	Use a library like `uuid` or `nanoid` for generating chat session IDs.
	•	Ensure the library is added to package.json dependencies.
	•	Mock UUID generation in tests for consistency.
	•	Type Safety:
	•	Ensure all types are properly imported from domain models (ChatSession, ChatMessage, ChatIntention).
	•	Use TypeScript's type system to catch errors at compile time.
	•	Validate intention values at runtime if needed (though TypeScript should catch most issues).
	•	Testing Strategy:
	•	Mock all external dependencies (LLM service, journal store, UUID library).
	•	Test store in isolation (don't hit real services or database).
	•	Use Vitest's mocking capabilities (vi.mock) to mock modules.
	•	Test both synchronous and asynchronous actions properly.
	•	Consider using Pinia's setActivePinia and createPinia for test setup.

⸻

Implementation Notes

	•	Store Structure Example:
	•	```typescript
	•	import { defineStore } from 'pinia'
	•	import type { ChatSession, ChatMessage, ChatIntention } from '@/domain/chatSession'
	•	import { sendMessage as sendLLMMessage } from '@/services/llmService'
	•	import { useJournalStore } from './journal.store'
	•	import { v4 as uuidv4 } from 'uuid'
	•	
	•	export const useChatStore = defineStore('chat', () => {
	•	  // State
	•	  const currentChatSession = ref<ChatSession | null>(null)
	•	  const isLoading = ref<boolean>(false)
	•	  const error = ref<string | null>(null)
	•	  const journalEntryId = ref<string | null>(null)
	•	
	•	  // Getters
	•	  const hasUnsavedMessages = computed(() => {
	•	    return currentChatSession.value !== null && 
	•	           currentChatSession.value.messages.length > 0
	•	  })
	•	
	•	  // Actions
	•	  async function startChatSession(entryId: string, intention: ChatIntention, customPrompt?: string) {
	•	    // Implementation
	•	  }
	•	
	•	  async function sendMessage(message: string) {
	•	    // Implementation
	•	  }
	•	
	•	  async function saveChatSession() {
	•	    // Implementation
	•	  }
	•	
	•	  function discardChatSession() {
	•	    // Implementation
	•	  }
	•	
	•	  async function loadChatSessionsForEntry(entryId: string): Promise<ChatSession[]> {
	•	    // Implementation
	•	  }
	•	
	•	  return {
	•	    // State
	•	    currentChatSession,
	•	    isLoading,
	•	    error,
	•	    journalEntryId,
	•	    // Getters
	•	    hasUnsavedMessages,
	•	    // Actions
	•	    startChatSession,
	•	    sendMessage,
	•	    saveChatSession,
	•	    discardChatSession,
	•	    loadChatSessionsForEntry
	•	  }
	•	})
	•	```
	•	Context Message Construction:
	•	The context message should be constructed when sending the first message. Example:
	•	```typescript
	•	async function sendMessage(message: string) {
	•	  if (!currentChatSession.value) {
	•	    error.value = 'No active chat session'
	•	    return
	•	  }
	•	
	•	  const journalStore = useJournalStore()
	•	  const entry = await journalStore.getEntryById(journalEntryId.value!)
	•	  
	•	  // Construct context message (only for first message)
	•	  const isFirstMessage = currentChatSession.value.messages.length === 0
	•	  let messagesToSend: Array<{role: string, content: string}> = []
	•	  
	•	  if (isFirstMessage) {
	•	    // Add system prompt (from Story 4, use placeholder for now)
	•	    messagesToSend.push({ role: 'system', content: getSystemPrompt(currentChatSession.value.intention) })
	•	    
	•	    // Add context message
	•	    const contextMessage = constructContextMessage(entry)
	•	    messagesToSend.push({ role: 'user', content: contextMessage })
	•	  }
	•	  
	•	  // Add conversation history
	•	  messagesToSend.push(...currentChatSession.value.messages.map(msg => ({
	•	    role: msg.role,
	•	    content: msg.content
	•	  })))
	•	  
	•	  // Add new user message
	•	  messagesToSend.push({ role: 'user', content: message })
	•	  
	•	  // Call LLM service
	•	  isLoading.value = true
	•	  error.value = null
	•	  
	•	  try {
	•	    const assistantResponse = await sendLLMMessage(messagesToSend)
	•	    
	•	    // Add user message and assistant response to session
	•	    currentChatSession.value.messages.push(
	•	      createChatMessage('user', message),
	•	      createChatMessage('assistant', assistantResponse)
	•	    )
	•	  } catch (err) {
	•	    // Handle error
	•	    error.value = getUserFriendlyError(err)
	•	  } finally {
	•	    isLoading.value = false
	•	  }
	•	}
	•	```
	•	File Organization:
	•	Store file: `src/stores/chat.store.ts`
	•	Test file: `src/stores/__tests__/chat.store.spec.ts`
	•	Follow existing naming conventions and file structure.
	•	Dependencies:
	•	Ensure `uuid` or `nanoid` package is added to package.json.
	•	Import types from domain models created in Story 2.
	•	Import LLM service from Story 1.
	•	Import journal store (assume it exists or will be created/updated as needed).
	•	Testing:
	•	Create comprehensive test file with mocked dependencies.
	•	Use Vitest's `vi.mock` to mock the LLM service and journal store.
	•	Use `setActivePinia(createPinia())` in test setup to initialize Pinia.
	•	Test each action thoroughly with both success and error scenarios.

